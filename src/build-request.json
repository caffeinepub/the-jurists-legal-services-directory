{
  "kind": "build_request",
  "title": "Add sitemap.xml and robots.txt for SEO completion",
  "projectName": "The Jurists",
  "priority": "normal",
  "requirements": [
    {
      "id": "REQ-31",
      "text": "Create a sitemap.xml file at frontend/public/sitemap.xml containing all public pages including home (/), about (/about), services (/services), service detail pages (/services/family-law, /services/corporate-law, /services/criminal-defense, /services/civil-law, /services/property-law, /services/real-estate-law, /services/intellectual-property, /services/employment-law, /services/litigation), blog list (/blog), blog detail pages (using article IDs from backend), contact (/contact), jurisdiction pages (/jurisdiction/hyderabad, /jurisdiction/secunderabad, /jurisdiction/rangareddy, /jurisdiction/cyberabad), with proper XML formatting, lastmod dates, changefreq values, and priority values according to page importance",
      "target": "frontend",
      "source": {
        "messageIds": [
          "msg-4"
        ]
      },
      "acceptanceCriteria": [
        "sitemap.xml file exists at frontend/public/sitemap.xml",
        "All public routes are included with full URLs",
        "XML follows proper sitemap protocol format",
        "Each URL entry includes loc, lastmod, changefreq, and priority tags",
        "Blog article URLs are dynamically included based on published articles",
        "File is accessible at the root domain when deployed"
      ]
    },
    {
      "id": "REQ-32",
      "text": "Create a robots.txt file at frontend/public/robots.txt that allows all search engine crawlers (User-agent: *), permits crawling of all content (Allow: /), includes a reference to the sitemap location (Sitemap: https://[domain]/sitemap.xml), and optionally includes crawl-delay directive if needed for rate limiting",
      "target": "frontend",
      "source": {
        "messageIds": [
          "msg-4"
        ]
      },
      "acceptanceCriteria": [
        "robots.txt file exists at frontend/public/robots.txt",
        "File allows all user agents to crawl all pages",
        "Sitemap URL reference is included in the file",
        "File is accessible at the root domain when deployed",
        "Syntax follows robots.txt standard format"
      ]
    }
  ],
  "constraints": [
    "Do not modify existing SEO implementations (meta tags, structured data, breadcrumbs, FAQs)",
    "Sitemap.xml must be a static file in frontend/public for direct access",
    "Robots.txt must be a static file in frontend/public for direct access",
    "Do not change backend schema or actor structure"
  ],
  "nonGoals": [
    "Dynamic sitemap generation from backend",
    "Automated sitemap updates on content changes",
    "XML sitemap index for multiple sitemaps",
    "Advanced robots.txt rules for specific crawlers"
  ],
  "imageRequirements": {
    "required": [],
    "edits": []
  }
}
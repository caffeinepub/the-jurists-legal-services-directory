{
  "kind": "implementation_plan",
  "version": "1.0",
  "title": "Add sitemap.xml and robots.txt for SEO completion",
  "requirements": [
    {
      "id": "REQ-31",
      "summary": "Create sitemap.xml with all public pages and blog articles",
      "acceptanceCriteria": [
        "sitemap.xml file exists at frontend/public/sitemap.xml",
        "All public routes are included with full URLs",
        "XML follows proper sitemap protocol format",
        "Each URL entry includes loc, lastmod, changefreq, and priority tags",
        "Blog article URLs are dynamically included based on published articles",
        "File is accessible at the root domain when deployed"
      ],
      "file_operations": [
        {
          "path": "frontend/public/sitemap.xml",
          "operation": "create",
          "description": "Create sitemap.xml file containing all public pages including home (/), about (/about), services (/services), service detail pages for all nine practice areas (/services/family-law, /services/corporate-law, /services/criminal-defense, /services/civil-law, /services/property-law, /services/real-estate-law, /services/intellectual-property, /services/employment-law, /services/litigation), blog list (/blog), all published blog article detail pages using article IDs from the backend, contact (/contact), and jurisdiction pages (/jurisdiction/hyderabad, /jurisdiction/secunderabad, /jurisdiction/rangareddy, /jurisdiction/cyberabad). Each URL entry must include loc with full domain URL, lastmod with ISO 8601 date format, changefreq (daily for blog/home, weekly for services, monthly for static pages), and priority (1.0 for home, 0.9 for services, 0.8 for service details and blog list, 0.7 for blog articles and jurisdictions, 0.6 for about/contact). Format the file with proper XML declaration, urlset namespace (http://www.sitemaps.org/schemas/sitemap/0.9), proper indentation, and valid XML structure. Use the current date for lastmod on static pages and actual publication dates for blog articles when available."
        }
      ]
    },
    {
      "id": "REQ-32",
      "summary": "Create robots.txt allowing all crawlers with sitemap reference",
      "acceptanceCriteria": [
        "robots.txt file exists at frontend/public/robots.txt",
        "File allows all user agents to crawl all pages",
        "Sitemap URL reference is included in the file",
        "File is accessible at the root domain when deployed",
        "Syntax follows robots.txt standard format"
      ],
      "file_operations": [
        {
          "path": "frontend/public/robots.txt",
          "operation": "create",
          "description": "Create robots.txt file with proper syntax allowing all search engine crawlers. Include 'User-agent: *' directive to target all bots, 'Allow: /' directive to permit crawling of all site content, and 'Sitemap:' directive with the full sitemap URL (https://[production-domain]/sitemap.xml). Use standard robots.txt formatting with one directive per line, blank lines between sections for readability, and comments explaining each section. Ensure the file uses plain text format with LF line endings and no BOM marker."
        }
      ]
    }
  ]
}